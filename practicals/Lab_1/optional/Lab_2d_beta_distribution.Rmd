---
title: "Introduction to Bayesian Statistics for Demographic Science"
output:
  pdf_document:
    toc: true
    toc_depth: '1'
  html_document:
    toc: true
    toc_depth: '1'
    df_print: paged
    number_sections: true
subtitle: 'Lab 1: Bayesian inference in Demography'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

>**Learning Objectives**<br>
>In this lab, you will learn :<br>
>- Bayesian inference in practice.<br>
>- The basics of `Stan` and `RStan`,<br>
>- Specifying a binomial model in `Stan` and `RStan`.<br>


# Preliminary work
First, create a folder for this Lab and change the working directory to this folder in R
```{r eval=F, echo=F, warning = F, message = F}
# set working directory 
setwd("") # insert path
```

Then, install and load the R packages needed for this session.
```{r eval=F, echo=F, warning = F, message = F}
packages <- c("tidyverse", "ggplot2", "ggpubr")

if(!require(packages))install.packages(packages)

lapply(packages, library, character.only = TRUE)
```

# Bayesian inference in practice (basics)

To illustrate how Bayesian inference works in practice, we are going to implement a binomial model which aims at estimating the probability of a baby being born in a given hospital. We want to know what the probability of the baby being a girl is. We can assume (and it is a reasonable assumption) that the outcome variable $Y$ which takes values 0 (boy) or 1 (girl) is generated by Bernoulli trials with some unknown parameter $\theta$ - the probability of the baby being a girl. This is well-described by a **binomial model**:  $P(girls=k, n|\theta)$.

  - It is nice because if we assume the prior to be Beta distribution, we can derive the posterior by hand.
  - Prior: $Beta(a,b)$ has expectation $a/(a+b)$
  - **posterior** is also Beta!
  - Posterior: $Beta(a+k, b+n-k)$, has expectation $(a+k)/(a+b+n)$
  - Note: when posterior looks like prior we call the prior **conjugate**

## Defining a prior distribution

What do (can) we know about the probability of the baby being a girl? Let's try some distributions. We know that the conjugate prior distribution is beta distribution.

```{r echo=F, warning = F, message = F}
priors <- expand_grid(shape1 = c(0.1, 0.5, 1:5), shape2 = c(0.1, 0.5, 1:5)) %>%
          expand(nesting(shape1, shape2),
                 x = seq(from = 0, to = 1, length.out = 1000)) %>% 
          mutate(a     = str_c("a = ", shape1),
                 b     = str_c("b = ", shape2),
                 group = rep(1:1000, each = 49))


ggplot(priors, aes(x = x, group = group)) +
    geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)),
            color = "grey50", size = 1.25) +
    scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +
    coord_cartesian(ylim = c(0, 3)) +
    labs(title = "Beta distributions for selecting a prior",
         y = expression(P(theta*"|"*a*", "*b))) +
    theme(panel.grid = element_blank()) +
    facet_grid(b~a) + theme_bw()
```

We could define an informative prior based on demographic knowledge with expectation $0.4854369$.

```{r echo=F, warning = F, message = F}
inf_prior <- data.frame( th = seq(0, 1, by = 0.001), 
                        prob = dbeta(seq(0, 1, by = 0.001), 
                                     1000,
                                     1000*(1-0.4854369)/0.4854369))


ggplot(inf_prior, aes(x = th, group = 1)) +
    geom_line(aes(y = prob)) +
    scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +
    labs(title = "Informative Beta distribution prior",
         y = expression(P(theta*"|"*a*", "*b))) +
    theme(panel.grid = element_blank()) + theme_bw()
```

## Simulating data
Based on  demographic knowledge, we simulate data on the first thousand of babies born in some hospital since the beginning of year 2017 (1 denotes a girl, 0 - a boy).

```{r echo=T, warning = F, message = F}
set.seed(202)
girl <- rbinom(n = 1000, size = 1, prob = 0.4854369)
girl

```

## Plotting prior, likelihood and posterior distributions

- What is the posterior distribution of parameter $\theta$ if we use conjugate priors?
- Plot prior distributions along the corresponding likelihood and posteriors. 
```{r echo=T, warning = F, message = F}
a <- 5
b <- 5
th <- seq(0, 1, by = 0.001)
k <- sum(girl[1:20]) # sample size:  20 observations
n <- length(girl[1:20])

distr <- data.frame(th)
distr$prior <- dbeta(df$th, a, b)
distr$likelihood <- dbeta(df$th, k + 1, n - k + 1)
distr$posterior <- dbeta(df$th, a + k, b + n - k)


d1 <- ggplot(data = distr, aes(x = th)) +
  geom_line(aes(y = prior, color = "Prior"), size = 1) +
  geom_line(aes(y = likelihood, color = "Likelihood"), size = 1) +
  geom_line(aes(y = posterior, color = "Posterior"), size = 1.5) +
  labs(title = "Beta Distribution: Prior, Likelihood, and Posterior (n=20)",
       x = "Theta",
       y = "Density") +
  theme_minimal() +
  scale_color_manual(values = c("Prior" = "darkgrey", "Likelihood" = "red", "Posterior" = "black"), name = "Distribution") +
  theme(legend.position = "bottom")
d1

```
## Non-informative vs informative priors
- How the posterior distribution of getting a girl changes given an informative prior with $a=1000, b=1000(1-0.4854369)/0.4854369)$?

```{r warning = F, message = F, eval = F}
th <- seq(0, 1, by = 0.001)
a <- 1000
b <- 1000 * (1 - 0.4854369) / 0.4854369 #informative prior with expectation 0.4854369

a=5
b=5
k <- sum(girl[1:20]) # sample size:  20 observations
n <- length(girl[1:20])

distr <- data.frame(th)
distr$prior <- dbeta(df$th, a, b)
distr$likelihood <- dbeta(df$th, k + 1, n - k + 1)
distr$posterior <- dbeta(df$th, a + k, b + n - k)


d1i <- ggplot(data = distr, aes(x = th)) +
  geom_line(aes(y = prior, color = "Prior"), size = 1) +
  geom_line(aes(y = likelihood, color = "Likelihood"), size = 1) +
  geom_line(aes(y = posterior, color = "Posterior"), size = 1.5) +
  labs(title = "Beta Distribution: (informative) prior, Likelihood, and Posterior (n=20)",
       x = "Theta",
       y = "Density") +
  theme_minimal() +
  scale_color_manual(values = c("Prior" = "darkgrey", "Likelihood" = "red", "Posterior" = "black"), name = "Distribution") +
  theme(legend.position = "bottom")
d1i

```
Arrange the plots in a single layout. 

```{r echo = T, warning = F, message = F}
comb_plot <- ggarrange(d1 + theme(plot.title = element_blank()), 
          d1i + theme(plot.title = element_blank()), ncol = 2, nrow = 1, common.legend = T)

annotate_figure(
  comb_plot,
  top = text_grob("Beta Distribution: Prior, Likelihood, and Posterior (n=20)", size = 14, face = "bold"),
  right = NULL) 

```

## Assessing the impact of different sample sizes 
Compute the mean for the first 20 observations of the simulated data. Then, calculate the same for $n=50$, $n=100$, and $n=1000$.

What is the frequentist estimator of Î¸ for each sample size? Compared it with the results obtained in the previous exercise with n=20.

Plot the data to see the distribution of the simulated data of different sample sizes (n=50, 100, 1000). Use the prior if the previous task (i.e. $a=5, b=5$).

```{r echo = T, warning = F, message = F}
# plotting prior
k <- sum(girl[1:50]) # sample size: 50 observations
n <- length(girl[1:50])


distr <- data.frame(th)
distr$prior <- dbeta(df$th, a, b)
distr$likelihood <- dbeta(df$th, k + 1, n - k + 1)
distr$posterior <- dbeta(df$th, a + k, b + n - k)


d2 <- ggplot(data = distr, aes(x = th)) +
  geom_line(aes(y = prior, color = "Prior"), size = 1) +
  geom_line(aes(y = likelihood, color = "Likelihood"), size = 1) +
  geom_line(aes(y = posterior, color = "Posterior"), size = 1.5) +
  labs(title = "Beta Distribution: Prior, Likelihood, and Posterior (n=50)",
       x = "Theta",
       y = "Density") +
  theme_minimal() +
  scale_color_manual(values = c("Prior" = "darkgrey", "Likelihood" = "red", "Posterior" = "black"), name = "Distribution") +
  theme(legend.position = "bottom")
d2
```

Keeping the same prior, increase sample size to $n=100$
```{r echo = T, warning = F, message = F}
# plotting prior
k <- sum(girl[1:100]) # sample size:  100 observations
n <- length(girl[1:100])


distr <- data.frame(th)
distr$prior <- dbeta(df$th, a, b)
distr$likelihood <- dbeta(df$th, k + 1, n - k + 1)
distr$posterior <- dbeta(df$th, a + k, b + n - k)


d3 <- ggplot(data = distr, aes(x = th)) +
  geom_line(aes(y = prior, color = "Prior"), size = 1) +
  geom_line(aes(y = likelihood, color = "Likelihood"), size = 1) +
  geom_line(aes(y = posterior, color = "Posterior"), size = 1.5) +
  labs(title = "Beta Distribution: Prior, Likelihood, and Posterior (n=100)",
       x = "Theta",
       y = "Density") +
  theme_minimal() +
  scale_color_manual(values = c("Prior" = "darkgrey", "Likelihood" = "red", "Posterior" = "black"), name = "Distribution") +
  theme(legend.position = "bottom")
d3

```
Now, use the whole sample (i.e. $n=1000$).
```{r echo = T, warning = F, message = F}
# plotting prior
k <- sum(girl) # sample size:  1000 observations
n <- length(girl)


distr <- data.frame(th)
distr$prior <- dbeta(df$th, a, b)
distr$likelihood <- dbeta(df$th, k + 1, n - k + 1)
distr$posterior <- dbeta(df$th, a + k, b + n - k)


d4 <- ggplot(data = distr, aes(x = th)) +
  geom_line(aes(y = prior, color = "Prior"), size = 1) +
  geom_line(aes(y = likelihood, color = "Likelihood"), size = 1) +
  geom_line(aes(y = posterior, color = "Posterior"), size = 1.5) +
  labs(title = "Beta Distribution: Prior, Likelihood, and Posterior (n=1000)",
       x = "Theta",
       y = "Density") +
  theme_minimal() +
  scale_color_manual(values = c("Prior" = "darkgrey", "Likelihood" = "red", "Posterior" = "black"), name = "Distribution") +
  theme(legend.position = "bottom")
d4
```

Arrange the plots in a single layout. 

```{r echo = T, warning = F, message = F}
comb_plot <- ggarrange(d1 + theme(plot.title = element_blank()), 
          d3 + theme(plot.title = element_blank()), 
          d4 + theme(plot.title = element_blank()), ncol = 2, nrow = 2, 
          labels = c("n=20", "n=50", "n=100", "n=1000"), common.legend = T)

annotate_figure(
  comb_plot,
  top = text_grob("Beta Distribution: Prior, Likelihood, and Posterior", size = 14, face = "bold"),
  right = NULL)
          d2 + theme(plot.title = element_blank()), 

```
- How does our inference change when we take sample size $n=50$, $n=100$, $n=1000$?

# Specifying a binomial model using `Stan` and `RStan`.
There is different software to carry out Bayesian inference. One of the most recent and popular software is [`Stan`](https://mc-stan.org/), which we are using in this lab for estimating the probability that aa newborn in a given hospital is a girl as in the previous section. 

## Basics of `Stan` and `RStan`
To use `Stan` in `R`, we need the to install and load the `RStan` package. Depending on your computer, you will be asked to install `Rtools`. Install it and be patient, given that it takes a little bit of time.
```{r warning = F, message = F, eval = T}
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies=TRUE)
library("rstan")
```
You can write a `Stan` program in a text editor, saving them with the extension `.stan` at the end of the file name (e.g. `model.stan`). You can also write a `Stan` program in RStudio as we are going to do in this lab.

## Writing a model in `Stan`
Stan works in six blocks of code, which correspond to `data`, `parameters`, `model`, `transformed data`, `transformed parameters`, and `generated quantities`. You need to define at least the three first blocks.

- The `data` block comprehends the input data. You need to define all the elements that your model requires. For our case, the elements are the sample size and the data denoting whether a girl or boy is born.

- The `parameters` block includes the parameters of our interest. In out case is $\theta$, which is bounded between $0$ and $1$ due to being a probability. 

- In the `model` block, you need to specify not only the likelihood for a model but also its priors.

```{r eval=F, echo=T}
stan_model = "
data {
  int<lower=0> N; //sample size with a minimum value of 0
  int k;          //data denoting whether a girl or boy is born
  }
transformed data {
  }
parameters {
  real<lower=0, upper=1>  theta;    //probability of having a girl
  }
transformed parameters {
  }
model {
  // Priors
  theta ~ beta(1,1);    #the same as uniform!
  
  // Likelihood
  k ~ binomial(N, theta);
  }
generated quantities{
}"
writeLines(stan_model, "./stan_model.stan")
```

There are three blocks which are empty in the `Stan` model above:

- The `transformed data` block, in which you can specify any transformation of the input data.

- The `transformed parameters` block which enables the definition of intermediate parameters when the likelihood requires to be specified in a different ways. 

- The `generated quantities` block, from which you can make predictions based on the resulting posterior.

## Formatting input data and initial values for `Stan` model
Once the model is specified, you need to format the input data to be used. `Stan` accepts data in a `list` format as follows  

```{r echo=T, eval=T}
data.inp <- list(k = sum(girl[1:20]), 
                 N = 20)         
```

To obtain easily convergence of the model, you can also define initial values. In our case, it is sensible to include $0.5$ as an initial value of our parameter $\theta$.

```{r echo=T, eval=T}
inits00 <- list(theta=0.5)
```

## Model estimation
```{r echo=T, eval=T}
fit.bin <- rstan::stan(  model_code = stan_model, 
                  data = data.inp, 
                  iter = 1000, 
                  thin=1, 
                  warmup = 500,
                  verbose = F, 
                  init = list(inits00, inits00),   # a list of two as two chains
                  chains = 2, cores = 2, 
                  seed = 26)
```

## Basic diagnostics
Before looking at the posterior distribution, you need to perform basic diagnostics. 

```{r echo=T, eval=T, warning = F, message=F}
plot(fit.bin, plotfun = "trace", pars = c("theta"), inc_warmup = F) 
plot(fit.bin, plotfun = "rhat", pars = c("theta"), inc_warmup = F)
```

```{r echo=T, eval=T, warning = F, message=F}
plot(fit.bin, plotfun = "dens", pars = c("theta"), inc_warmup = F) + scale_x_continuous(limits = c(0, 1))
plot(fit.bin, plotfun = "plot", pars = c("theta"), inc_warmup = F) + scale_x_continuous(limits = c(0, 1))
```





